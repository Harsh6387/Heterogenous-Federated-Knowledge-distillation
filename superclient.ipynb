{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9ee1b-76d5-4e6c-94d0-ce3c3bd3969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import flwr as fl\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Define the LungCancerModel class\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomCrop(224, padding=4),    # This will work as RandomCrop after resizing\n",
    "    transforms.RandomHorizontalFlip(),        # This applies to training images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),  # CIFAR-100 mean and std\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),  # CIFAR-100 mean and std\n",
    "])\n",
    "\n",
    "# Load ImageNet dataset\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "num_examples = {\"trainset\" : len(train_dataset), \"testset\" : len(test_dataset)}\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Set device (GPU or CPU)\n",
    "\n",
    "\n",
    "# Load pre-trained ResNet-50 model\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained ResNet-50 model\n",
    "net = models.vgg19(pretrained=False)  # Set pretrained=True to use pre-trained weights\n",
    "net.classifier[6] = torch.nn.Linear(in_features=4096, out_features=100) \n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Load the saved weights from the .npz file\n",
    "npz_file_path = \"./round-3-weights.npz\"  # Replace with your file path\n",
    "loaded = np.load(npz_file_path)\n",
    "\n",
    "net = net.to(device)\n",
    "model = net\n",
    "\n",
    "# Step 4: Load the weights into the model's state_dict\n",
    "state_dict = OrderedDict()\n",
    "for i, (key, value) in enumerate(zip(model.state_dict().keys(), loaded.values())):\n",
    "    state_dict[key] = torch.tensor(value)  # Convert np.ndarray to torch.Tensor\n",
    "\n",
    "# Load the state_dict into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "print(\"Model loaded with weights from npz file.\")\n",
    "\n",
    "# Check if the weights are loaded correctly\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Weights: {param.data.cpu().numpy().flatten()[:5]}\")  # Move tensor to CPU before converting to NumPy\n",
    "\n",
    "\n",
    "# Check if the weights are loaded correctly\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Weights: {param.data.numpy().flatten()[:5]}\")  # Printing first 5 weights of each layer for verification\n",
    "\n",
    "\n",
    "\n",
    "teacher=model\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "\n",
    "\n",
    "student_model = models.vgg11(pretrained=False)\n",
    "\n",
    "\n",
    "# Modify the classifier to have 100 output classes for CIFAR-100\n",
    "student_model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=100)  # CIFAR-100 has 100 classes\n",
    " # CIFAR-10 has 10 classes\n",
    "student_model.to(device)\n",
    "\n",
    "\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "def distillation_loss(student_outputs, teacher_outputs, labels, T, alpha):\n",
    "    \"\"\"\n",
    "    Compute the distillation loss using KL Divergence and Cross Entropy.\n",
    "    :param student_outputs: Outputs from the student model\n",
    "    :param teacher_outputs: Outputs from the teacher model\n",
    "    :param labels: True labels\n",
    "    :param T: Temperature for softening probabilities\n",
    "    :param alpha: Weight for combining losses\n",
    "    :return: Combined loss\n",
    "    \"\"\"\n",
    "    # Soft targets loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(nn.functional.log_softmax(student_outputs / T, dim=1),\n",
    "                               nn.functional.softmax(teacher_outputs / T, dim=1)) * (T * T)\n",
    "    # True labels loss\n",
    "    hard_loss = criterion(student_outputs, labels)\n",
    "    # Combined loss\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "\n",
    "def train_student(teacher_model, student_model, train_loader, epochs=10, T=2.0, alpha=0.5):\n",
    "    teacher_model.to(device)\n",
    "    teacher_model.eval()\n",
    "   # Teacher model is in eval mode\n",
    "    student_model.to(device)\n",
    "    student_model.train()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=0.01)# Student model is in training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Get outputs from teacher model (no gradient computation)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(images)\n",
    "\n",
    "            # Get outputs from student model\n",
    "            student_outputs = student_model(images)\n",
    "\n",
    "            # Compute the distillation loss\n",
    "            loss = distillation_loss(student_outputs, teacher_outputs, labels, T, alpha)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(student_outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print epoch results\n",
    "        accuracy = 100. * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Example usage\n",
    "# train_losses = train_student_with_distillation(teacher, net , train_loader, num_epochs=10)\n",
    "\n",
    "def test(net, test_loader):\n",
    "    \"\"\"Validate the model on the test set.\"\"\"\n",
    "    net.to(device)\n",
    "    net.eval()  # Set model to evaluation mode\n",
    "    criterion = nn.CrossEntropyLoss().to(device)  # Ensure criterion is on the same device\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()  # Sum the loss for averaging later\n",
    "\n",
    "            # Get predictions and calculate correct predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)  # Total number of labels\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = running_loss / len(test_loader)  # Average loss over all batches\n",
    "    accuracy = 100. * correct / total  # Correct predictions percentage\n",
    "\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "class CifarClient(fl.client.NumPyClient):\n",
    "    def get_parameters(self, config):\n",
    "        return [val.cpu().numpy() for _, val in student_model.state_dict().items()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(student_model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        student_model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        train_student(teacher, student_model , train_loader, epochs=15,T=2.0, alpha=0.5)\n",
    "        return self.get_parameters(config={}), num_examples[\"trainset\"], {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        loss, accuracy = test(student_model, test_loader)\n",
    "        return float(loss), num_examples[\"testset\"], {\"accuracy\": float(accuracy)}\n",
    "    \n",
    "\n",
    "fl.client.start_numpy_client(server_address=\"127.0.0.1:8083\", client=CifarClient(),)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
